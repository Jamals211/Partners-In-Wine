{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the requests library.\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import the requests library.\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "# Dependencies for the wine API\n",
    "import urllib\n",
    "import json\n",
    "# Import the API key.\n",
    "from config import Token_NOAA\n",
    "from config import API_Token\n",
    "import calendar\n",
    "#Suppress Warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "#Display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/crvaden/NOAA_API_v2\n",
    "# https://towardsdatascience.com/getting-weather-data-in-3-easy-steps-8dc10cc5c859\n",
    "# https://cran.r-project.org/web/packages/rnoaa/rnoaa.pdf\n",
    "# file:///C:/Users/15124/Downloads/GHCND_documentation.pdf\n",
    "# https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data By Zip Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "napadf = pd.DataFrame()\n",
    "states = {\n",
    "    'Napa': {\n",
    "        'zip': '95472',\n",
    "        'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "        },\n",
    "#     'Walla': {\n",
    "#         'zip': '99362',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Columbia': {\n",
    "#         'zip': '98813',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Sonoma': {\n",
    "#         'zip': '95476',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Santa': {\n",
    "#         'zip': '95062',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Yakima': {\n",
    "#         'zip': '98903',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Dundee': {\n",
    "#         'zip': '97045',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Willamette': {\n",
    "#         'zip': '97013',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0bd51ecca66e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:{zip}&datatypeid=TMAX&datatypeid=TMIN&units=standard&limit=1000&startdate='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'-01-01&enddate='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'-12-31'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'token'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mToken_NOAA\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#Load JSON data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m#     print(d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#e = json.loads(p.text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    small_df = pd.DataFrame()\n",
    "    for year in states[state]['years']:\n",
    "        zip = states[state]['zip']\n",
    "        df_test = pd.DataFrame()\n",
    "        #Print out 'working on year' to idenfity if script is running correctly\n",
    "#         print('working on year '+str(year))\n",
    "        #make the api call for temp and precipitation\n",
    "        r = requests.get(f'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:{zip}&datatypeid=TMAX&datatypeid=TMIN&units=standard&limit=1000&startdate='+year+'-01-01&enddate='+year+'-12-31', headers={'token':Token_NOAA})\n",
    "        #Load JSON data\n",
    "        d = json.loads(r.text)\n",
    "        #     print(d)\n",
    "        #e = json.loads(p.text)\n",
    "        items_MAX = [item for item in d['results'] if item['datatype']=='TMAX']\n",
    "        items_MIN = [item for item in d['results'] if item['datatype']=='TMIN']\n",
    "        # #get the date field from all average temperature readings\n",
    "        dates_temp_MAX = [item['date'].split('T')[0] for item in items_MAX]\n",
    "        dates_temp_MIN = [item['date'].split('T')[0] for item in items_MIN]\n",
    "        df_test['date'] = dates_temp_MAX\n",
    "        df_test['avgMaxTemp'+state] = np.nan\n",
    "        for item in items_MAX:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMaxTemp'+state]] = item['value']\n",
    "        df_test['avgMinTemp'+state] = np.nan\n",
    "        for item in items_MIN:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMinTemp'+state]] = item['value']\n",
    "        small_df = pd.concat([small_df,df_test])\n",
    "    if napadf.empty:\n",
    "        napadf = small_df\n",
    "    else:\n",
    "        napadf = pd.merge(napadf,small_df)\n",
    "napadf.to_csv( 'Napa_Max_Min.csv', index = False)\n",
    "# napadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "walladf = pd.DataFrame()\n",
    "states = {\n",
    "#     'Napa': {\n",
    "#         'zip': '95472',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "    'Walla': {\n",
    "        'zip': '99362',\n",
    "        'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "        },\n",
    "#     'Columbia': {\n",
    "#         'zip': '98813',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Sonoma': {\n",
    "#         'zip': '95476',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Santa': {\n",
    "#         'zip': '95062',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Yakima': {\n",
    "#         'zip': '98903',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Dundee': {\n",
    "#         'zip': '97045',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Willamette': {\n",
    "#         'zip': '97013',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cbd2e69c1e80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:{zip}&datatypeid=TMAX&datatypeid=TMIN&units=standard&limit=1000&startdate='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'-01-01&enddate='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'-12-31'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'token'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mToken_NOAA\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#Load JSON data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m#     print(d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#e = json.loads(p.text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    small_df = pd.DataFrame()\n",
    "    for year in states[state]['years']:\n",
    "        zip = states[state]['zip']\n",
    "        df_test = pd.DataFrame()\n",
    "        #Print out 'working on year' to idenfity if script is running correctly\n",
    "#         print('working on year '+str(year))\n",
    "        #make the api call for temp and precipitation\n",
    "        r = requests.get(f'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:{zip}&datatypeid=TMAX&datatypeid=TMIN&units=standard&limit=1000&startdate='+year+'-01-01&enddate='+year+'-12-31', headers={'token':Token_NOAA})\n",
    "        #Load JSON data\n",
    "        d = json.loads(r.text)\n",
    "        #     print(d)\n",
    "        #e = json.loads(p.text)\n",
    "        items_MAX = [item for item in d['results'] if item['datatype']=='TMAX']\n",
    "        items_MIN = [item for item in d['results'] if item['datatype']=='TMIN']\n",
    "        # #get the date field from all average temperature readings\n",
    "        dates_temp_MAX = [item['date'].split('T')[0] for item in items_MAX]\n",
    "        dates_temp_MIN = [item['date'].split('T')[0] for item in items_MIN]\n",
    "        df_test['date'] = dates_temp_MAX\n",
    "        df_test['avgMaxTemp'+state] = np.nan\n",
    "        for item in items_MAX:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMaxTemp'+state]] = item['value']\n",
    "        df_test['avgMinTemp'+state] = np.nan\n",
    "        for item in items_MIN:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMinTemp'+state]] = item['value']\n",
    "        small_df = pd.concat([small_df,df_test])\n",
    "    if walladf.empty:\n",
    "        walladf = small_df\n",
    "    else:\n",
    "        walladf = pd.merge(walladf,small_df)\n",
    "walladf.to_csv( 'Walla_Max_Min.csv', index = False)\n",
    "# walladf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columbiadf = pd.DataFrame()\n",
    "states = {\n",
    "#     'Napa': {\n",
    "#         'zip': '95472',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Walla': {\n",
    "#         'zip': '99362',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "    'Columbia': {\n",
    "        'zip': '98813',\n",
    "        'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "        },\n",
    "#     'Sonoma': {\n",
    "#         'zip': '95476',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Santa': {\n",
    "#         'zip': '95062',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Yakima': {\n",
    "#         'zip': '98903',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Dundee': {\n",
    "#         'zip': '97045',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Willamette': {\n",
    "#         'zip': '97013',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    small_df = pd.DataFrame()\n",
    "    for year in states[state]['years']:\n",
    "        zip = states[state]['zip']\n",
    "        df_test = pd.DataFrame()\n",
    "        #Print out 'working on year' to idenfity if script is running correctly\n",
    "#         print('working on year '+str(year))\n",
    "        #make the api call for temp and precipitation\n",
    "        r = requests.get(f'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:{zip}&datatypeid=TMAX&datatypeid=TMIN&units=standard&limit=1000&startdate='+year+'-01-01&enddate='+year+'-12-31', headers={'token':Token_NOAA})\n",
    "        #Load JSON data\n",
    "        d = json.loads(r.text)\n",
    "        #     print(d)\n",
    "        #e = json.loads(p.text)\n",
    "        items_MAX = [item for item in d['results'] if item['datatype']=='TMAX']\n",
    "        items_MIN = [item for item in d['results'] if item['datatype']=='TMIN']\n",
    "        # #get the date field from all average temperature readings\n",
    "        dates_temp_MAX = [item['date'].split('T')[0] for item in items_MAX]\n",
    "        dates_temp_MIN = [item['date'].split('T')[0] for item in items_MIN]\n",
    "        df_test['date'] = dates_temp_MAX\n",
    "        df_test['avgMaxTemp'+state] = np.nan\n",
    "        for item in items_MAX:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMaxTemp'+state]] = item['value']\n",
    "        df_test['avgMinTemp'+state] = np.nan\n",
    "        for item in items_MIN:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMinTemp'+state]] = item['value']\n",
    "        small_df = pd.concat([small_df,df_test])\n",
    "    if columbiadf.empty:\n",
    "        columbiadf = small_df\n",
    "    else:\n",
    "        columbiadf = pd.merge(columbiadf,small_df)\n",
    "columbiadf.to_csv( 'Columbia_Max_Min.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonomadf = pd.DataFrame()\n",
    "states = {\n",
    "#     'Napa': {\n",
    "#         'zip': '95472',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Walla': {\n",
    "#         'zip': '99362',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Columbia': {\n",
    "#         'zip': '98813',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "    'Sonoma': {\n",
    "        'zip': '95476',\n",
    "        'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "        },\n",
    "#     'Santa': {\n",
    "#         'zip': '95062',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Yakima': {\n",
    "#         'zip': '98903',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Dundee': {\n",
    "#         'zip': '97045',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Willamette': {\n",
    "#         'zip': '97013',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    small_df = pd.DataFrame()\n",
    "    for year in states[state]['years']:\n",
    "        zip = states[state]['zip']\n",
    "        df_test = pd.DataFrame()\n",
    "        #Print out 'working on year' to idenfity if script is running correctly\n",
    "#         print('working on year '+str(year))\n",
    "        #make the api call for temp and precipitation\n",
    "        r = requests.get(f'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:{zip}&datatypeid=TMAX&datatypeid=TMIN&units=standard&limit=1000&startdate='+year+'-01-01&enddate='+year+'-12-31', headers={'token':Token_NOAA})\n",
    "        #Load JSON data\n",
    "        d = json.loads(r.text)\n",
    "        #     print(d)\n",
    "        #e = json.loads(p.text)\n",
    "        items_MAX = [item for item in d['results'] if item['datatype']=='TMAX']\n",
    "        items_MIN = [item for item in d['results'] if item['datatype']=='TMIN']\n",
    "        # #get the date field from all average temperature readings\n",
    "        dates_temp_MAX = [item['date'].split('T')[0] for item in items_MAX]\n",
    "        dates_temp_MIN = [item['date'].split('T')[0] for item in items_MIN]\n",
    "        df_test['date'] = dates_temp_MAX\n",
    "        df_test['avgMaxTemp'+state] = np.nan\n",
    "        for item in items_MAX:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMaxTemp'+state]] = item['value']\n",
    "        df_test['avgMinTemp'+state] = np.nan\n",
    "        for item in items_MIN:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMinTemp'+state]] = item['value']\n",
    "        small_df = pd.concat([small_df,df_test])\n",
    "    if sonomadf.empty:\n",
    "        sonomadf = small_df\n",
    "    else:\n",
    "        sonomadf = pd.merge(sonomadf,small_df)\n",
    "sonomadf.to_csv( 'Sonoma_Max_Min.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santadf = pd.DataFrame()\n",
    "states = {\n",
    "#     'Napa': {\n",
    "#         'zip': '95472',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Walla': {\n",
    "#         'zip': '99362',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Columbia': {\n",
    "#         'zip': '98813',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Sonoma': {\n",
    "#         'zip': '95476',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "    'Santa': {\n",
    "        'zip': '95062',\n",
    "        'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "        },\n",
    "#     'Yakima': {\n",
    "#         'zip': '98903',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Dundee': {\n",
    "#         'zip': '97045',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Willamette': {\n",
    "#         'zip': '97013',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    small_df = pd.DataFrame()\n",
    "    for year in states[state]['years']:\n",
    "        zip = states[state]['zip']\n",
    "        df_test = pd.DataFrame()\n",
    "        #Print out 'working on year' to idenfity if script is running correctly\n",
    "#         print('working on year '+str(year))\n",
    "        #make the api call for temp and precipitation\n",
    "        r = requests.get(f'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:{zip}&datatypeid=TMAX&datatypeid=TMIN&units=standard&limit=1000&startdate='+year+'-01-01&enddate='+year+'-12-31', headers={'token':Token_NOAA})\n",
    "        #Load JSON data\n",
    "        d = json.loads(r.text)\n",
    "        #     print(d)\n",
    "        #e = json.loads(p.text)\n",
    "        items_MAX = [item for item in d['results'] if item['datatype']=='TMAX']\n",
    "        items_MIN = [item for item in d['results'] if item['datatype']=='TMIN']\n",
    "        # #get the date field from all average temperature readings\n",
    "        dates_temp_MAX = [item['date'].split('T')[0] for item in items_MAX]\n",
    "        dates_temp_MIN = [item['date'].split('T')[0] for item in items_MIN]\n",
    "        df_test['date'] = dates_temp_MAX\n",
    "        df_test['avgMaxTemp'+state] = np.nan\n",
    "        for item in items_MAX:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMaxTemp'+state]] = item['value']\n",
    "        df_test['avgMinTemp'+state] = np.nan\n",
    "        for item in items_MIN:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMinTemp'+state]] = item['value']\n",
    "        small_df = pd.concat([small_df,df_test])\n",
    "    if santadf.empty:\n",
    "        santadf = small_df\n",
    "    else:\n",
    "        santadf = pd.merge(sonomadf,small_df)\n",
    "santadf.to_csv( 'Santa_Max_Min.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yakimadf = pd.DataFrame()\n",
    "states = {\n",
    "#     'Napa': {\n",
    "#         'zip': '95472',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Walla': {\n",
    "#         'zip': '99362',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Columbia': {\n",
    "#         'zip': '98813',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Sonoma': {\n",
    "#         'zip': '95476',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Santa': {\n",
    "#         'zip': '95062',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "    'Yakima': {\n",
    "        'zip': '98903',\n",
    "        'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "        },\n",
    "#     'Dundee': {\n",
    "#         'zip': '97045',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Willamette': {\n",
    "#         'zip': '97013',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    small_df = pd.DataFrame()\n",
    "    for year in states[state]['years']:\n",
    "        zip = states[state]['zip']\n",
    "        df_test = pd.DataFrame()\n",
    "        #Print out 'working on year' to idenfity if script is running correctly\n",
    "#         print('working on year '+str(year))\n",
    "        #make the api call for temp and precipitation\n",
    "        r = requests.get(f'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:{zip}&datatypeid=TMAX&datatypeid=TMIN&units=standard&limit=1000&startdate='+year+'-01-01&enddate='+year+'-12-31', headers={'token':Token_NOAA})\n",
    "        #Load JSON data\n",
    "        d = json.loads(r.text)\n",
    "        #     print(d)\n",
    "        #e = json.loads(p.text)\n",
    "        items_MAX = [item for item in d['results'] if item['datatype']=='TMAX']\n",
    "        items_MIN = [item for item in d['results'] if item['datatype']=='TMIN']\n",
    "        # #get the date field from all average temperature readings\n",
    "        dates_temp_MAX = [item['date'].split('T')[0] for item in items_MAX]\n",
    "        dates_temp_MIN = [item['date'].split('T')[0] for item in items_MIN]\n",
    "        df_test['date'] = dates_temp_MAX\n",
    "        df_test['avgMaxTemp'+state] = np.nan\n",
    "        for item in items_MAX:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMaxTemp'+state]] = item['value']\n",
    "        df_test['avgMinTemp'+state] = np.nan\n",
    "        for item in items_MIN:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMinTemp'+state]] = item['value']\n",
    "        small_df = pd.concat([small_df,df_test])\n",
    "    if yakimadf.empty:\n",
    "        yakimadf = small_df\n",
    "    else:\n",
    "        yakimadf = pd.merge(yakimadf,small_df)\n",
    "yakimadf.to_csv( 'Yakima_Max_Min.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dundeedf = pd.DataFrame()\n",
    "states = {\n",
    "#     'Napa': {\n",
    "#         'zip': '95472',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Walla': {\n",
    "#         'zip': '99362',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Columbia': {\n",
    "#         'zip': '98813',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Sonoma': {\n",
    "#         'zip': '95476',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Santa': {\n",
    "#         'zip': '95062',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Yakima': {\n",
    "#         'zip': '98903',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "    'Dundee': {\n",
    "        'zip': '97045',\n",
    "        'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "        },\n",
    "#     'Willamette': {\n",
    "#         'zip': '97013',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    small_df = pd.DataFrame()\n",
    "    for year in states[state]['years']:\n",
    "        zip = states[state]['zip']\n",
    "        df_test = pd.DataFrame()\n",
    "        #Print out 'working on year' to idenfity if script is running correctly\n",
    "#         print('working on year '+str(year))\n",
    "        #make the api call for temp and precipitation\n",
    "        r = requests.get(f'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:{zip}&datatypeid=TMAX&datatypeid=TMIN&units=standard&limit=1000&startdate='+year+'-01-01&enddate='+year+'-12-31', headers={'token':Token_NOAA})\n",
    "        #Load JSON data\n",
    "        d = json.loads(r.text)\n",
    "        #     print(d)\n",
    "        #e = json.loads(p.text)\n",
    "        items_MAX = [item for item in d['results'] if item['datatype']=='TMAX']\n",
    "        items_MIN = [item for item in d['results'] if item['datatype']=='TMIN']\n",
    "        # #get the date field from all average temperature readings\n",
    "        dates_temp_MAX = [item['date'].split('T')[0] for item in items_MAX]\n",
    "        dates_temp_MIN = [item['date'].split('T')[0] for item in items_MIN]\n",
    "        df_test['date'] = dates_temp_MAX\n",
    "        df_test['avgMaxTemp'+state] = np.nan\n",
    "        for item in items_MAX:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMaxTemp'+state]] = item['value']\n",
    "        df_test['avgMinTemp'+state] = np.nan\n",
    "        for item in items_MIN:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMinTemp'+state]] = item['value']\n",
    "        small_df = pd.concat([small_df,df_test])\n",
    "    if dundeedf.empty:\n",
    "        dundeedf = small_df\n",
    "    else:\n",
    "        dundeedf = pd.merge(dundeedf,small_df)\n",
    "dundeedf.to_csv( 'Dundee_Max_Min.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "willamettedf = pd.DataFrame()\n",
    "states = {\n",
    "#     'Napa': {\n",
    "#         'zip': '95472',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Walla': {\n",
    "#         'zip': '99362',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Columbia': {\n",
    "#         'zip': '98813',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Sonoma': {\n",
    "#         'zip': '95476',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Santa': {\n",
    "#         'zip': '95062',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Yakima': {\n",
    "#         'zip': '98903',\n",
    "#         'years':['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "#     'Dundee': {\n",
    "#         'zip': '97045',\n",
    "#         'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "#         },\n",
    "    'Willamette': {\n",
    "        'zip': '97013',\n",
    "        'years': ['1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017']\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    small_df = pd.DataFrame()\n",
    "    for year in states[state]['years']:\n",
    "        zip = states[state]['zip']\n",
    "        df_test = pd.DataFrame()\n",
    "        #Print out 'working on year' to idenfity if script is running correctly\n",
    "#         print('working on year '+str(year))\n",
    "        #make the api call for temp and precipitation\n",
    "        r = requests.get(f'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:{zip}&datatypeid=TMAX&datatypeid=TMIN&units=standard&limit=1000&startdate='+year+'-01-01&enddate='+year+'-12-31', headers={'token':Token_NOAA})\n",
    "        #Load JSON data\n",
    "        d = json.loads(r.text)\n",
    "        #     print(d)\n",
    "        #e = json.loads(p.text)\n",
    "        items_MAX = [item for item in d['results'] if item['datatype']=='TMAX']\n",
    "        items_MIN = [item for item in d['results'] if item['datatype']=='TMIN']\n",
    "        # #get the date field from all average temperature readings\n",
    "        dates_temp_MAX = [item['date'].split('T')[0] for item in items_MAX]\n",
    "        dates_temp_MIN = [item['date'].split('T')[0] for item in items_MIN]\n",
    "        df_test['date'] = dates_temp_MAX\n",
    "        df_test['avgMaxTemp'+state] = np.nan\n",
    "        for item in items_MAX:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMaxTemp'+state]] = item['value']\n",
    "        df_test['avgMinTemp'+state] = np.nan\n",
    "        for item in items_MIN:\n",
    "            date = item['date'].split('T')[0]\n",
    "            df_test.loc[df_test['date'] == date, ['avgMinTemp'+state]] = item['value']\n",
    "        small_df = pd.concat([small_df,df_test])\n",
    "    if willamettedf.empty:\n",
    "        willamettedf = small_df\n",
    "    else:\n",
    "        willamettedf = pd.merge(willamettedf,small_df)\n",
    "willamettedf.to_csv( 'Will_Max_Min.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "willamettedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
